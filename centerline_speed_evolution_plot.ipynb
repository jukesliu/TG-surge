{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab glacier centerline speeds and the plot the speed evolution time series\n",
    "\n",
    "_Last modified by jukesliu@u.boisestate.edu on 2022-05-02._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from ordered_set import OrderedSet # pip install ordered-set\n",
    "import cmocean\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.colors import LogNorm\n",
    "import datetime\n",
    "\n",
    "from additional_functions import mytomd, unique_date_df, vector_magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = '/Users/jukesliu/Documents/TURNER/DATA/VELOCITY_MAPS/' # SET WORKING DIRECTORY\n",
    "# the root folder that holds centerline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/jukesliu/Documents/TURNER/DATA/VELOCITY_MAPS/custom_autoRIFT_cline_data/centerline_data_n.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# grab reference centerline distances from a data file:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m sorted_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbasepath\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcustom_autoRIFT_cline_data/centerline_data_n.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# PATH TO THE SAR CSV\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# may need to adjust column\u001b[39;00m\n\u001b[1;32m      4\u001b[0m dists \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(OrderedSet(sorted_df\u001b[38;5;241m.\u001b[39mdist_km))[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m      5\u001b[0m dists\u001b[38;5;241m.\u001b[39msort()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/newautoriftenv/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/newautoriftenv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:586\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    571\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    572\u001b[0m     dialect,\n\u001b[1;32m    573\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    583\u001b[0m )\n\u001b[1;32m    584\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/newautoriftenv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:482\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    479\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/newautoriftenv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:811\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwds:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 811\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/newautoriftenv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1040\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1037\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown engine: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (valid options are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1038\u001b[0m     )\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m# error: Too many arguments for \"ParserBase\"\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/newautoriftenv/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:51\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     48\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musecols\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musecols\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# open handles\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open_handles\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Have to pass int, would break tests using TextReader directly otherwise :(\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/newautoriftenv/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py:222\u001b[0m, in \u001b[0;36mParserBase._open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_handles\u001b[39m(\u001b[38;5;28mself\u001b[39m, src: FilePathOrBuffer, kwds: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m    Let the readers open IOHandles after they are done with their potential raises.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/newautoriftenv/lib/python3.9/site-packages/pandas/io/common.py:701\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    700\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    710\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/jukesliu/Documents/TURNER/DATA/VELOCITY_MAPS/custom_autoRIFT_cline_data/centerline_data_n.csv'"
     ]
    }
   ],
   "source": [
    "# grab reference centerline distances from a data file:\n",
    "sorted_df = pd.read_csv(basepath+'custom_autoRIFT_cline_data/centerline_data_n.csv', # PATH TO THE SAR CSV\n",
    "                    usecols=[2,3,4,5,6,7,8,9,10,11]) # may need to adjust column\n",
    "dists = list(OrderedSet(sorted_df.dist_km))[1:]\n",
    "dists.sort()\n",
    "print(dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1A) Read in individual centerline profiles and combine\n",
    "\n",
    "When speeds along centerline are stored in a folder containing a CSV file with the data for each date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/jukesliu/Documents/TURNER/DATA/VELOCITY_MAPS/custom_autoRIFT_cline_data/filtered-south/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m cfilespath \u001b[38;5;241m=\u001b[39m basepath\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom_autoRIFT_cline_data/filtered-south/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# ENTER PATH TO THE FOLDER CONTAINING THE PROFILES\u001b[39;00m\n\u001b[1;32m      3\u001b[0m interp_dfs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfilespath\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprofile\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[1;32m      6\u001b[0m         ds1 \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m8\u001b[39m:\u001b[38;5;241m16\u001b[39m]; ds2 \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m17\u001b[39m:\u001b[38;5;241m25\u001b[39m]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/jukesliu/Documents/TURNER/DATA/VELOCITY_MAPS/custom_autoRIFT_cline_data/filtered-south/'"
     ]
    }
   ],
   "source": [
    "cfilespath = basepath+'custom_autoRIFT_cline_data/filtered-south/' # ENTER PATH TO THE FOLDER CONTAINING THE PROFILES\n",
    "\n",
    "interp_dfs = []\n",
    "for file in os.listdir(cfilespath):\n",
    "    if file.startswith('profile') and file.endswith('.csv') and 'S' not in file:\n",
    "        ds1 = file[8:16]; ds2 = file[17:25]\n",
    "        profile_df = pd.read_csv(cfilespath+file, names=['dist_km','vmag_md']) # read in data\n",
    "        \n",
    "        # correct distance values if not correct\n",
    "        maxdist = np.nanmax(profile_df.dist_km) # grab the maximum distance value \n",
    "        if  maxdist < 29.54666: # distance should go out to 29.54 km\n",
    "            newdistkm = np.flip(np.array(profile_df.dist_km * 29.54666/maxdist)) # rescale to 29.54 km\n",
    "            profile_df.dist_km = newdistkm # replace in dataframe\n",
    "        \n",
    "#         downsample to SAR dist values\n",
    "        sample_indices = np.round(np.linspace(0,len(profile_df)-1,len(dists))) # grab sampling indexes for dists\n",
    "        distances = dists\n",
    "#         sample_indices = np.round(np.linspace(0,len(profile_df)-1,63)) # grab full profile\n",
    "#         distances = np.flip(profile_df.dist_km[sample_indices])\n",
    "        \n",
    "        v_interp = np.flip(profile_df.vmag_md[sample_indices]) # grab down-sampled spped values\n",
    "    \n",
    "        # fill in ds1 and ds2 columns\n",
    "        ds1s = np.full(np.size(v_interp),ds1) \n",
    "        ds2s = np.full(np.size(v_interp),ds2)\n",
    "\n",
    "        # enter into dataframe\n",
    "        interp_df = pd.DataFrame(list(zip(ds1s,ds2s,distances,v_interp)),columns=['ds1','ds2','dist_km','vmag'])\n",
    "        \n",
    "        # calculate datetimes\n",
    "        interp_df['ds1'] = pd.to_datetime(ds1s, format='%Y%m%d')\n",
    "        interp_df['ds2'] = pd.to_datetime(ds2s, format='%Y%m%d')\n",
    "        interp_dfs.append(interp_df)\n",
    "\n",
    "# enter into one dataframe\n",
    "interp_total = pd.concat(interp_dfs).sort_values(by=['ds1','ds2','dist_km'])\n",
    "interp_total = interp_total.drop_duplicates()\n",
    "interp_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds1</th>\n",
       "      <th>ds2</th>\n",
       "      <th>dist_km</th>\n",
       "      <th>vmag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>2013-06-10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>2013-06-10</td>\n",
       "      <td>0.35</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>2013-06-10</td>\n",
       "      <td>0.60</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>2013-06-10</td>\n",
       "      <td>0.85</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>2013-06-10</td>\n",
       "      <td>1.10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ds1        ds2  dist_km  vmag\n",
       "0 2013-04-14 2013-06-10     0.10   NaN\n",
       "1 2013-04-14 2013-06-10     0.35   NaN\n",
       "2 2013-04-14 2013-06-10     0.60   NaN\n",
       "3 2013-04-14 2013-06-10     0.85   NaN\n",
       "4 2013-04-14 2013-06-10     1.10   NaN"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NEW DATA\n",
    "cfilespath = basepath+'/custom_autoRIFT_cline_data/filtered/' # ENTER PATH TO THE FOLDER CONTAINING THE PROFILES\n",
    "\n",
    "interp_dfs = []\n",
    "for file in os.listdir(cfilespath):\n",
    "        ds1 = file.split('_')[0]; ds2 = file.split('_')[1]\n",
    "        profile_df = pd.read_csv(cfilespath+file) # read in data\n",
    "        \n",
    "        v_interp = vector_magnitude(profile_df.vx, profile_df.vy)/365\n",
    "        distances = profile_df.dist_m/1000\n",
    "    \n",
    "        # fill in ds1 and ds2 columns\n",
    "        ds1s = np.full(np.size(v_interp),ds1) \n",
    "        ds2s = np.full(np.size(v_interp),ds2)\n",
    "\n",
    "        # enter into dataframe\n",
    "        interp_df = pd.DataFrame(list(zip(ds1s,ds2s,distances,v_interp)),columns=['ds1','ds2','dist_km','vmag'])\n",
    "        \n",
    "        # calculate datetimes\n",
    "        interp_df['ds1'] = pd.to_datetime(ds1s, format='%Y%m%d')\n",
    "        interp_df['ds2'] = pd.to_datetime(ds2s, format='%Y%m%d')\n",
    "        interp_dfs.append(interp_df)\n",
    "\n",
    "# enter into one dataframe\n",
    "interp_total = pd.concat(interp_dfs).sort_values(by=['ds1','ds2','dist_km'])\n",
    "interp_total = interp_total.drop_duplicates()\n",
    "interp_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds1</th>\n",
       "      <th>ds2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>2013-06-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-06-10</td>\n",
       "      <td>2013-06-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-06-17</td>\n",
       "      <td>2013-07-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-07-12</td>\n",
       "      <td>2013-07-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-07-28</td>\n",
       "      <td>2013-08-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>2021-08-15</td>\n",
       "      <td>2021-08-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>2021-08-30</td>\n",
       "      <td>2021-09-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>2021-09-07</td>\n",
       "      <td>2021-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>2021-09-19</td>\n",
       "      <td>2021-09-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>2021-09-22</td>\n",
       "      <td>2021-10-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>205 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ds1        ds2\n",
       "0   2013-04-14 2013-06-10\n",
       "1   2013-06-10 2013-06-17\n",
       "2   2013-06-17 2013-07-12\n",
       "3   2013-07-12 2013-07-28\n",
       "4   2013-07-28 2013-08-13\n",
       "..         ...        ...\n",
       "200 2021-08-15 2021-08-30\n",
       "201 2021-08-30 2021-09-07\n",
       "202 2021-09-07 2021-09-19\n",
       "203 2021-09-19 2021-09-22\n",
       "204 2021-09-22 2021-10-04\n",
       "\n",
       "[205 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grab the unique dates\n",
    "df2 = unique_date_df(interp_total,'ds1','ds2')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(205, 121)\n"
     ]
    }
   ],
   "source": [
    "# initialize speed grid\n",
    "speed_grid = np.zeros((len(df2), len(distances)))\n",
    "\n",
    "# fill in speed grid with distances\n",
    "rown = 0\n",
    "for idx, row in df2.iterrows():\n",
    "    # grab the dates\n",
    "    d1 = row.ds1; d2 = row.ds2\n",
    "    \n",
    "    # grab the part of the df matching those dates\n",
    "    date_df = interp_total[interp_total.ds1 == d1]\n",
    "    date_df = date_df[date_df.ds2 == d2]\n",
    "    date_df.reset_index(drop=True, inplace=True)\n",
    "    date_df = date_df.drop_duplicates(subset=['dist_km'],keep='last') # drop duplications\n",
    "    \n",
    "    # append into row of speed grid\n",
    "    speed_grid[rown,:] = list(date_df.vmag) # add speed along centerline to speed_grid\n",
    "    rown += 1\n",
    "print(speed_grid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in temporal gaps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-04-14 2013-06-10\n",
      "2013-06-10 2013-06-17\n",
      "2013-06-17 2013-07-12\n",
      "2013-07-12 2013-07-28\n",
      "2013-07-28 2013-08-13\n",
      "2013-08-13 2013-10-07\n",
      "\n",
      "Gap between 2013-10-07 00:00:00 and 2014-02-28 00:00:00\n",
      "\n",
      "2014-02-28 2014-03-25\n",
      "2014-03-25 2014-04-01\n",
      "2014-04-01 2014-04-10\n",
      "2014-04-10 2014-04-26\n",
      "2014-04-26 2014-05-03\n",
      "2014-05-03 2014-06-04\n",
      "2014-06-04 2014-07-22\n",
      "2014-07-22 2014-07-31\n",
      "2014-07-31 2014-08-23\n",
      "2014-08-23 2014-09-01\n",
      "2014-09-01 2014-09-24\n",
      "\n",
      "Gap between 2014-09-24 00:00:00 and 2015-03-28 00:00:00\n",
      "\n",
      "2015-03-28 2015-04-13\n",
      "2015-04-13 2015-05-06\n",
      "2015-05-06 2015-05-15\n",
      "2015-05-15 2015-05-22\n",
      "2015-05-22 2015-05-31\n",
      "2015-05-31 2015-06-23\n",
      "2015-06-23 2015-07-09\n",
      "2015-07-09 2015-07-25\n",
      "2015-07-25 2015-08-03\n",
      "\n",
      "Gap between 2015-08-03 00:00:00 and 2016-02-18 00:00:00\n",
      "\n",
      "2016-02-18 2016-03-14\n",
      "2016-03-14 2016-03-21\n",
      "2016-03-21 2016-05-08\n",
      "2016-05-08 2016-06-09\n",
      "2016-06-09 2016-07-11\n",
      "2016-07-11 2016-08-28\n",
      "2016-08-28 2016-09-29\n",
      "2016-09-29 2016-10-08\n",
      "2016-10-08 2016-10-24\n",
      "\n",
      "Gap between 2016-10-24 00:00:00 and 2017-02-05 00:00:00\n",
      "\n",
      "2017-02-05 2017-03-07\n",
      "\n",
      "Gap between 2017-03-07 00:00:00 and 2017-02-20 00:00:00\n",
      "\n",
      "2017-02-20 2017-03-17\n",
      "\n",
      "Gap between 2017-03-17 00:00:00 and 2017-03-07 00:00:00\n",
      "\n",
      "2017-03-07 2017-04-16\n",
      "\n",
      "Gap between 2017-04-16 00:00:00 and 2017-03-17 00:00:00\n",
      "\n",
      "2017-03-17 2017-03-24\n",
      "2017-03-24 2017-05-11\n",
      "\n",
      "Gap between 2017-05-11 00:00:00 and 2017-04-16 00:00:00\n",
      "\n",
      "2017-04-16 2017-04-23\n",
      "2017-04-23 2017-05-06\n",
      "2017-05-06 2017-05-26\n",
      "\n",
      "Gap between 2017-05-26 00:00:00 and 2017-05-11 00:00:00\n",
      "\n",
      "2017-05-11 2017-05-27\n",
      "\n",
      "Gap between 2017-05-27 00:00:00 and 2017-05-26 00:00:00\n",
      "\n",
      "2017-05-26 2017-06-02\n",
      "\n",
      "Gap between 2017-06-02 00:00:00 and 2017-05-27 00:00:00\n",
      "\n",
      "2017-05-27 2017-07-23\n",
      "\n",
      "Gap between 2017-07-23 00:00:00 and 2017-06-02 00:00:00\n",
      "\n",
      "2017-06-02 2017-07-05\n",
      "2017-07-05 2017-07-12\n",
      "\n",
      "Gap between 2017-07-12 00:00:00 and 2017-07-23 00:00:00\n",
      "\n",
      "2017-07-23 2017-08-08\n",
      "2017-08-08 2017-08-15\n",
      "\n",
      "Gap between 2017-08-15 00:00:00 and 2017-10-28 00:00:00\n",
      "\n",
      "2017-10-28 2017-11-04\n",
      "2017-11-04 2017-11-09\n",
      "2017-11-09 2017-11-14\n",
      "\n",
      "Gap between 2017-11-14 00:00:00 and 2018-01-21 00:00:00\n",
      "\n",
      "2018-01-21 2018-01-28\n",
      "2018-01-28 2018-02-02\n",
      "2018-02-02 2018-02-10\n",
      "2018-02-10 2018-02-17\n",
      "2018-02-17 2018-02-25\n",
      "2018-02-25 2018-03-02\n",
      "2018-03-02 2018-03-22\n",
      "2018-03-22 2018-03-29\n",
      "2018-03-29 2018-04-01\n",
      "2018-04-01 2018-04-11\n",
      "2018-04-11 2018-04-16\n",
      "2018-04-16 2018-05-06\n",
      "2018-05-06 2018-05-16\n",
      "2018-05-16 2018-05-31\n",
      "2018-05-31 2018-07-02\n",
      "2018-07-02 2018-07-05\n",
      "2018-07-05 2018-07-20\n",
      "2018-07-20 2018-07-30\n",
      "2018-07-30 2018-08-19\n",
      "2018-08-19 2018-08-31\n",
      "2018-08-31 2018-09-03\n",
      "2018-09-03 2018-09-08\n",
      "\n",
      "Gap between 2018-09-08 00:00:00 and 2018-09-03 00:00:00\n",
      "\n",
      "2018-09-03 2018-09-12\n",
      "\n",
      "Gap between 2018-09-12 00:00:00 and 2018-09-08 00:00:00\n",
      "\n",
      "2018-09-08 2018-09-13\n",
      "\n",
      "Gap between 2018-09-13 00:00:00 and 2018-09-12 00:00:00\n",
      "\n",
      "2018-09-12 2018-09-19\n",
      "\n",
      "Gap between 2018-09-19 00:00:00 and 2018-09-13 00:00:00\n",
      "\n",
      "2018-09-13 2018-09-18\n",
      "2018-09-18 2018-09-30\n",
      "2018-09-30 2018-10-08\n",
      "2018-10-08 2018-11-02\n",
      "2018-11-02 2018-11-07\n",
      "\n",
      "Gap between 2018-11-07 00:00:00 and 2019-01-18 00:00:00\n",
      "\n",
      "2019-01-18 2019-01-31\n",
      "2019-01-31 2019-02-07\n",
      "2019-02-07 2019-02-10\n",
      "2019-02-10 2019-02-15\n",
      "\n",
      "Gap between 2019-02-15 00:00:00 and 2019-02-10 00:00:00\n",
      "\n",
      "2019-02-10 2019-02-19\n",
      "\n",
      "Gap between 2019-02-19 00:00:00 and 2019-02-15 00:00:00\n",
      "\n",
      "2019-02-15 2019-02-22\n",
      "\n",
      "Gap between 2019-02-22 00:00:00 and 2019-02-19 00:00:00\n",
      "\n",
      "2019-02-19 2019-02-26\n",
      "\n",
      "Gap between 2019-02-26 00:00:00 and 2019-02-22 00:00:00\n",
      "\n",
      "2019-02-22 2019-02-25\n",
      "2019-02-25 2019-03-07\n",
      "\n",
      "Gap between 2019-03-07 00:00:00 and 2019-02-26 00:00:00\n",
      "\n",
      "2019-02-26 2019-03-07\n",
      "\n",
      "Gap between 2019-03-07 00:00:00 and 2019-03-04 00:00:00\n",
      "\n",
      "2019-03-04 2019-03-07\n",
      "2019-03-07 2019-03-27\n",
      "\n",
      "Gap between 2019-03-27 00:00:00 and 2019-03-07 00:00:00\n",
      "\n",
      "2019-03-07 2019-03-30\n",
      "\n",
      "Gap between 2019-03-30 00:00:00 and 2019-03-27 00:00:00\n",
      "\n",
      "2019-03-27 2019-04-01\n",
      "\n",
      "Gap between 2019-04-01 00:00:00 and 2019-03-30 00:00:00\n",
      "\n",
      "2019-03-30 2019-04-15\n",
      "\n",
      "Gap between 2019-04-15 00:00:00 and 2019-04-01 00:00:00\n",
      "\n",
      "2019-04-01 2019-04-13\n",
      "2019-04-13 2019-04-26\n",
      "\n",
      "Gap between 2019-04-26 00:00:00 and 2019-04-15 00:00:00\n",
      "\n",
      "2019-04-15 2019-04-24\n",
      "2019-04-24 2019-05-01\n",
      "\n",
      "Gap between 2019-05-01 00:00:00 and 2019-04-26 00:00:00\n",
      "\n",
      "2019-04-26 2019-05-01\n",
      "2019-05-01 2019-05-13\n",
      "\n",
      "Gap between 2019-05-13 00:00:00 and 2019-05-01 00:00:00\n",
      "\n",
      "2019-05-01 2019-06-27\n",
      "\n",
      "Gap between 2019-06-27 00:00:00 and 2019-05-13 00:00:00\n",
      "\n",
      "2019-05-13 2019-06-07\n",
      "2019-06-07 2019-06-10\n",
      "2019-06-10 2019-06-20\n",
      "2019-06-20 2019-06-25\n",
      "2019-06-25 2019-06-30\n",
      "\n",
      "Gap between 2019-06-30 00:00:00 and 2019-06-27 00:00:00\n",
      "\n",
      "2019-06-27 2019-07-04\n",
      "\n",
      "Gap between 2019-07-04 00:00:00 and 2019-06-30 00:00:00\n",
      "\n",
      "2019-06-30 2019-07-05\n",
      "\n",
      "Gap between 2019-07-05 00:00:00 and 2019-07-04 00:00:00\n",
      "\n",
      "2019-07-04 2019-08-21\n",
      "\n",
      "Gap between 2019-08-21 00:00:00 and 2019-07-05 00:00:00\n",
      "\n",
      "2019-07-05 2019-07-10\n",
      "2019-07-10 2019-07-22\n",
      "2019-07-22 2019-07-30\n",
      "2019-07-30 2019-08-04\n",
      "2019-08-04 2019-08-09\n",
      "2019-08-09 2019-08-19\n",
      "2019-08-19 2019-08-29\n",
      "\n",
      "Gap between 2019-08-29 00:00:00 and 2019-08-21 00:00:00\n",
      "\n",
      "2019-08-21 2019-08-30\n",
      "\n",
      "Gap between 2019-08-30 00:00:00 and 2019-08-29 00:00:00\n",
      "\n",
      "2019-08-29 2019-09-05\n",
      "\n",
      "Gap between 2019-09-05 00:00:00 and 2019-08-30 00:00:00\n",
      "\n",
      "2019-08-30 2019-09-06\n",
      "\n",
      "Gap between 2019-09-06 00:00:00 and 2019-09-05 00:00:00\n",
      "\n",
      "2019-09-05 2019-09-08\n",
      "\n",
      "Gap between 2019-09-08 00:00:00 and 2019-09-06 00:00:00\n",
      "\n",
      "2019-09-06 2019-09-15\n",
      "\n",
      "Gap between 2019-09-15 00:00:00 and 2019-09-08 00:00:00\n",
      "\n",
      "2019-09-08 2019-09-15\n",
      "2019-09-15 2019-10-03\n",
      "\n",
      "Gap between 2019-10-03 00:00:00 and 2019-09-15 00:00:00\n",
      "\n",
      "2019-09-15 2019-10-08\n",
      "\n",
      "Gap between 2019-10-08 00:00:00 and 2019-10-03 00:00:00\n",
      "\n",
      "2019-10-03 2019-10-08\n",
      "2019-10-08 2019-10-13\n",
      "\n",
      "Gap between 2019-10-13 00:00:00 and 2019-10-08 00:00:00\n",
      "\n",
      "2019-10-08 2019-11-09\n",
      "\n",
      "Gap between 2019-11-09 00:00:00 and 2019-10-13 00:00:00\n",
      "\n",
      "2019-10-13 2019-10-20\n",
      "2019-10-20 2019-10-25\n",
      "2019-10-25 2019-11-09\n",
      "\n",
      "Gap between 2019-11-09 00:00:00 and 2020-01-16 00:00:00\n",
      "\n",
      "2020-01-16 2020-02-02\n",
      "2020-02-02 2020-03-06\n",
      "\n",
      "Gap between 2020-03-06 00:00:00 and 2020-02-13 00:00:00\n",
      "\n",
      "2020-02-13 2020-04-01\n",
      "\n",
      "Gap between 2020-04-01 00:00:00 and 2020-03-06 00:00:00\n",
      "\n",
      "2020-03-06 2020-03-11\n",
      "2020-03-11 2020-03-23\n",
      "2020-03-23 2020-03-26\n",
      "2020-03-26 2020-03-30\n",
      "2020-03-30 2020-04-02\n",
      "\n",
      "Gap between 2020-04-02 00:00:00 and 2020-04-01 00:00:00\n",
      "\n",
      "2020-04-01 2020-04-10\n",
      "\n",
      "Gap between 2020-04-10 00:00:00 and 2020-04-01 00:00:00\n",
      "\n",
      "2020-04-01 2020-04-17\n",
      "\n",
      "Gap between 2020-04-17 00:00:00 and 2020-04-02 00:00:00\n",
      "\n",
      "2020-04-02 2020-04-05\n",
      "2020-04-05 2020-04-10\n",
      "2020-04-10 2020-04-17\n",
      "2020-04-17 2020-04-20\n",
      "\n",
      "Gap between 2020-04-20 00:00:00 and 2020-04-17 00:00:00\n",
      "\n",
      "2020-04-17 2020-05-12\n",
      "\n",
      "Gap between 2020-05-12 00:00:00 and 2020-04-20 00:00:00\n",
      "\n",
      "2020-04-20 2020-04-25\n",
      "2020-04-25 2020-05-02\n",
      "\n",
      "Gap between 2020-05-02 00:00:00 and 2020-04-27 00:00:00\n",
      "\n",
      "2020-04-27 2020-05-10\n",
      "\n",
      "Gap between 2020-05-10 00:00:00 and 2020-05-02 00:00:00\n",
      "\n",
      "2020-05-02 2020-05-05\n",
      "2020-05-05 2020-05-10\n",
      "\n",
      "Gap between 2020-05-10 00:00:00 and 2020-05-05 00:00:00\n",
      "\n",
      "2020-05-05 2020-05-15\n",
      "\n",
      "Gap between 2020-05-15 00:00:00 and 2020-05-10 00:00:00\n",
      "\n",
      "2020-05-10 2020-05-15\n",
      "\n",
      "Gap between 2020-05-15 00:00:00 and 2020-05-12 00:00:00\n",
      "\n",
      "2020-05-12 2020-05-28\n",
      "\n",
      "Gap between 2020-05-28 00:00:00 and 2020-05-15 00:00:00\n",
      "\n",
      "2020-05-15 2020-05-27\n",
      "2020-05-27 2020-06-04\n",
      "\n",
      "Gap between 2020-06-04 00:00:00 and 2020-05-28 00:00:00\n",
      "\n",
      "2020-05-28 2020-06-04\n",
      "2020-06-04 2020-06-09\n",
      "\n",
      "Gap between 2020-06-09 00:00:00 and 2020-06-04 00:00:00\n",
      "\n",
      "2020-06-04 2020-07-15\n",
      "\n",
      "Gap between 2020-07-15 00:00:00 and 2020-06-04 00:00:00\n",
      "\n",
      "2020-06-04 2020-07-22\n",
      "\n",
      "Gap between 2020-07-22 00:00:00 and 2020-06-09 00:00:00\n",
      "\n",
      "2020-06-09 2020-06-26\n",
      "2020-06-26 2020-07-01\n",
      "2020-07-01 2020-07-16\n",
      "2020-07-16 2020-07-26\n",
      "2020-07-26 2020-07-29\n",
      "2020-07-29 2020-08-18\n",
      "2020-08-18 2020-09-04\n",
      "2020-09-04 2020-09-14\n",
      "2020-09-14 2020-09-24\n",
      "2020-09-24 2020-10-17\n",
      "\n",
      "Gap between 2020-10-17 00:00:00 and 2020-10-10 00:00:00\n",
      "\n",
      "2020-10-10 2020-11-20\n",
      "\n",
      "Gap between 2020-11-20 00:00:00 and 2021-02-04 00:00:00\n",
      "\n",
      "2021-02-04 2021-02-11\n",
      "2021-02-11 2021-04-02\n",
      "\n",
      "Gap between 2021-04-02 00:00:00 and 2021-02-15 00:00:00\n",
      "\n",
      "2021-02-15 2021-03-03\n",
      "\n",
      "Gap between 2021-03-03 00:00:00 and 2021-02-15 00:00:00\n",
      "\n",
      "2021-02-15 2021-03-12\n",
      "\n",
      "Gap between 2021-03-12 00:00:00 and 2021-03-03 00:00:00\n",
      "\n",
      "2021-03-03 2021-03-12\n",
      "2021-03-12 2021-03-19\n",
      "2021-03-19 2021-03-28\n",
      "\n",
      "Gap between 2021-03-28 00:00:00 and 2021-04-02 00:00:00\n",
      "\n",
      "2021-04-02 2021-04-15\n",
      "2021-04-15 2021-04-22\n",
      "2021-04-22 2021-04-25\n",
      "2021-04-25 2021-05-02\n",
      "2021-05-02 2021-05-05\n",
      "2021-05-05 2021-05-17\n",
      "2021-05-17 2021-05-24\n",
      "2021-05-24 2021-06-01\n",
      "2021-06-01 2021-06-04\n",
      "2021-06-04 2021-06-09\n",
      "2021-06-09 2021-06-14\n",
      "2021-06-14 2021-06-19\n",
      "2021-06-19 2021-07-01\n",
      "2021-07-01 2021-07-04\n",
      "2021-07-04 2021-07-16\n",
      "2021-07-16 2021-07-19\n",
      "2021-07-19 2021-07-29\n",
      "2021-07-29 2021-08-03\n",
      "2021-08-03 2021-08-15\n",
      "2021-08-15 2021-08-30\n",
      "2021-08-30 2021-09-07\n",
      "2021-09-07 2021-09-19\n",
      "2021-09-19 2021-09-22\n",
      "72 gaps\n"
     ]
    }
   ],
   "source": [
    "date_ends = np.array(list((zip(df2.ds1, df2.ds2)))).flatten() # intersperse ds1 and ds2\n",
    "\n",
    "fill_dfs = []\n",
    "counter = 0\n",
    "# identify temporal gaps\n",
    "for i in np.arange(0, len(date_ends),2):\n",
    "    if i+2 < len(date_ends):\n",
    "        # grab the two dates for that speed_grid()\n",
    "        date1_start = date_ends[i]\n",
    "        date1_end = date_ends[i+1]\n",
    "        date2_start = date_ends[i+2]\n",
    "        print(str(date1_start)[0:10],str(date1_end)[0:10]) # print the date start and end\n",
    "    \n",
    "        if not date1_end == date2_start: # if the end date and the next start date don't match\n",
    "            print()\n",
    "            print('Gap between', date1_end, 'and', date2_start) # Gap found\n",
    "            print()\n",
    "\n",
    "            # OPTION 1: fill in all gaps gap with Nans\n",
    "            ds1s = np.full(np.size(distances),date1_end) \n",
    "            ds2s = np.full(np.size(distances),date2_start)\n",
    "            nans = np.empty(len(distances)); nans[:] = np.nan # create list of nans to fill\n",
    "            fill_df = pd.DataFrame(list(zip(ds1s, ds2s,distances,nans)),columns=interp_total.columns)\n",
    "            fill_dfs.append(fill_df)\n",
    "            \n",
    "#             # OPTION 2: fill in the gaps with SAR data\n",
    "#             sorted_df = pd.read_csv(basepath+'ASF_autoRIFT/centerline_data_n.csv', # PATH TO THE SAR CSV\n",
    "#                                     usecols=[2,3,4,5,6,7,8,9,10,11]) # may need to adjust column\n",
    "#             # convert dates to datetime objects using pd\n",
    "#             sorted_df['mid_date'] = pd.to_datetime(sorted_df.mid_date, format='%Y%m%d')\n",
    "#             sorted_df['ds1'] = pd.to_datetime(sorted_df.ds1, format='%Y-%m-%d')\n",
    "#             sorted_df['ds2'] = pd.to_datetime(sorted_df.ds2, format='%Y-%m-%d')\n",
    "#             sorted_df['vmag'] = mytomd(sorted_df.vmag) # convert velocities to m/day\n",
    "#             sorted_df['v_error'] = mytomd(sorted_df.v_error)\n",
    "#             sorted_df = sorted_df.drop_duplicates(subset=['ds1','lat'],keep='first') # drop overlapping dates\n",
    "\n",
    "#             # grab each unique date pair\n",
    "#             dates_df = unique_date_df(sorted_df,'ds1','ds2')\n",
    "#             gap_df = dates_df[(dates_df.ds1 >= date1_end) & (dates_df.ds2 <= date2_start)] # find those in gap\n",
    "#             if len(gap_df) > 0: # fill in gaps with SAR data\n",
    "#                 print('Fill with:')\n",
    "#                 for idx, row in gap_df.iterrows():\n",
    "#                     print(row.ds1, row.ds2)\n",
    "#                     df = sorted_df[(sorted_df.ds1 == row.ds1) & (sorted_df.ds2 == row.ds2)]\n",
    "#                     fill_df = df[[\"ds1\", \"ds2\",\"dist_km\", \"vmag\"]]\n",
    "#                     fill_dfs.append(fill_df)\n",
    "#             else: # if no optical data found, fill in with nans\n",
    "#                 print(\"Fill with nans\")\n",
    "#                 ds1s = np.full(np.size(dists),date1_end) \n",
    "#                 ds2s = np.full(np.size(dists),date2_start)\n",
    "#                 nans = np.empty(len(dists)); nans[:] = np.nan # create list of nans to fill\n",
    "#                 fill_df = pd.DataFrame(list(zip(ds1s, ds2s, dists,nans)),columns=interp_total.columns)\n",
    "#                 fill_dfs.append(fill_df)\n",
    "                \n",
    "                \n",
    "            counter+=1  # count the gaps      \n",
    "print(counter, 'gaps')\n",
    "\n",
    "# add gap-filling data back into the dataframe\n",
    "interp_total_filled = pd.concat([interp_total,pd.concat(fill_dfs)]).sort_values(by=['ds1','ds2','dist_km'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1B) Read in combined centerline data file\n",
    "\n",
    "When speeds along centerline for all dates have been combined into one large file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>PSx</th>\n",
       "      <th>vmag</th>\n",
       "      <th>v_error</th>\n",
       "      <th>dist_km</th>\n",
       "      <th>ds1</th>\n",
       "      <th>ds2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-139.942202</td>\n",
       "      <td>60.119742</td>\n",
       "      <td>-3.296994e+06</td>\n",
       "      <td>132.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2016-10-20</td>\n",
       "      <td>2016-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-139.937770</td>\n",
       "      <td>60.120118</td>\n",
       "      <td>-3.296972e+06</td>\n",
       "      <td>62.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.249907</td>\n",
       "      <td>2016-10-20</td>\n",
       "      <td>2016-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-139.933279</td>\n",
       "      <td>60.120221</td>\n",
       "      <td>-3.296983e+06</td>\n",
       "      <td>23.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.499879</td>\n",
       "      <td>2016-10-20</td>\n",
       "      <td>2016-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-139.928796</td>\n",
       "      <td>60.120056</td>\n",
       "      <td>-3.297024e+06</td>\n",
       "      <td>78.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.749845</td>\n",
       "      <td>2016-10-20</td>\n",
       "      <td>2016-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-139.924315</td>\n",
       "      <td>60.119860</td>\n",
       "      <td>-3.297069e+06</td>\n",
       "      <td>65.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.999845</td>\n",
       "      <td>2016-10-20</td>\n",
       "      <td>2016-11-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          lon        lat           PSx   vmag  v_error   dist_km         ds1  \\\n",
       "0 -139.942202  60.119742 -3.296994e+06  132.0     54.0  0.000000  2016-10-20   \n",
       "1 -139.937770  60.120118 -3.296972e+06   62.0     46.0  0.249907  2016-10-20   \n",
       "2 -139.933279  60.120221 -3.296983e+06   23.0     57.0  0.499879  2016-10-20   \n",
       "3 -139.928796  60.120056 -3.297024e+06   78.0     61.0  0.749845  2016-10-20   \n",
       "4 -139.924315  60.119860 -3.297069e+06   65.0     51.0  0.999845  2016-10-20   \n",
       "\n",
       "          ds2  \n",
       "0  2016-11-13  \n",
       "1  2016-11-13  \n",
       "2  2016-11-13  \n",
       "3  2016-11-13  \n",
       "4  2016-11-13  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# csvpath = basepath+'ASF_autoRIFT/centerline_data_n.csv' # ENTER PATH TO THE CSV FILE WITH ALL DATA COMBINED\n",
    "# csvpath = '/Users/jukesliu/Documents/TURNER/DATA/VELOCITY_MAPS/its_live/notebooks/'\n",
    "csvpath = '/Users/jukesliu/Documents/TURNER/DATA/VELOCITY_MAPS/ASF_autoRIFT/'\n",
    "csvpath += 'centerline_data_s.csv'\n",
    "\n",
    "# read into a dataframe\n",
    "sorted_df = pd.read_csv(csvpath, usecols=[2,3,4,6,7,8,10,11]) # ADJUST COLUMNS AS NEEDED, DROP MID_DATE\n",
    "sorted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>PSx</th>\n",
       "      <th>vmag</th>\n",
       "      <th>v_error</th>\n",
       "      <th>dist_km</th>\n",
       "      <th>ds1</th>\n",
       "      <th>ds2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-139.942202</td>\n",
       "      <td>60.119742</td>\n",
       "      <td>-3.296994e+06</td>\n",
       "      <td>0.361644</td>\n",
       "      <td>0.147945</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2016-10-20</td>\n",
       "      <td>2016-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-139.937770</td>\n",
       "      <td>60.120118</td>\n",
       "      <td>-3.296972e+06</td>\n",
       "      <td>0.169863</td>\n",
       "      <td>0.126027</td>\n",
       "      <td>0.249907</td>\n",
       "      <td>2016-10-20</td>\n",
       "      <td>2016-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-139.933279</td>\n",
       "      <td>60.120221</td>\n",
       "      <td>-3.296983e+06</td>\n",
       "      <td>0.063014</td>\n",
       "      <td>0.156164</td>\n",
       "      <td>0.499879</td>\n",
       "      <td>2016-10-20</td>\n",
       "      <td>2016-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-139.928796</td>\n",
       "      <td>60.120056</td>\n",
       "      <td>-3.297024e+06</td>\n",
       "      <td>0.213699</td>\n",
       "      <td>0.167123</td>\n",
       "      <td>0.749845</td>\n",
       "      <td>2016-10-20</td>\n",
       "      <td>2016-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-139.924315</td>\n",
       "      <td>60.119860</td>\n",
       "      <td>-3.297069e+06</td>\n",
       "      <td>0.178082</td>\n",
       "      <td>0.139726</td>\n",
       "      <td>0.999845</td>\n",
       "      <td>2016-10-20</td>\n",
       "      <td>2016-11-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          lon        lat           PSx      vmag   v_error   dist_km  \\\n",
       "0 -139.942202  60.119742 -3.296994e+06  0.361644  0.147945  0.000000   \n",
       "1 -139.937770  60.120118 -3.296972e+06  0.169863  0.126027  0.249907   \n",
       "2 -139.933279  60.120221 -3.296983e+06  0.063014  0.156164  0.499879   \n",
       "3 -139.928796  60.120056 -3.297024e+06  0.213699  0.167123  0.749845   \n",
       "4 -139.924315  60.119860 -3.297069e+06  0.178082  0.139726  0.999845   \n",
       "\n",
       "         ds1        ds2  \n",
       "0 2016-10-20 2016-11-13  \n",
       "1 2016-10-20 2016-11-13  \n",
       "2 2016-10-20 2016-11-13  \n",
       "3 2016-10-20 2016-11-13  \n",
       "4 2016-10-20 2016-11-13  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert dates to datetime objects using pd\n",
    "sorted_df['ds1'] = pd.to_datetime(sorted_df.ds1, format='%Y-%m-%d')\n",
    "sorted_df['ds2'] = pd.to_datetime(sorted_df.ds2, format='%Y-%m-%d')\n",
    "\n",
    "# convert velocities to m/day\n",
    "sorted_df['vmag'] = mytomd(sorted_df.vmag)\n",
    "sorted_df['v_error'] = mytomd(sorted_df.v_error)\n",
    "\n",
    "# drop overlapping dates!\n",
    "sorted_df = sorted_df.drop_duplicates(subset=['ds1','lat'],keep='first') \n",
    "sorted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds1</th>\n",
       "      <th>ds2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-10-20</td>\n",
       "      <td>2016-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-11-13</td>\n",
       "      <td>2016-12-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-12-07</td>\n",
       "      <td>2016-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>2017-01-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-24</td>\n",
       "      <td>2017-02-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ds1        ds2\n",
       "0 2016-10-20 2016-11-13\n",
       "1 2016-11-13 2016-12-07\n",
       "2 2016-12-07 2016-12-31\n",
       "3 2016-12-31 2017-01-24\n",
       "4 2017-01-24 2017-02-17"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grab each unique date pair\n",
    "df2 = unique_date_df(sorted_df,'ds1','ds2')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97, 98)\n"
     ]
    }
   ],
   "source": [
    "# initialize speed grid\n",
    "# speed_grid = np.zeros((len(df2), len(dists)))\n",
    "speed_grid = np.zeros((len(df2), len(list(set(sorted_df.dist_km)))))\n",
    "print(speed_grid.shape)\n",
    "\n",
    "# fill in speed grid with speed values for each date pair\n",
    "rown = 0\n",
    "for idx, row in df2.iterrows():\n",
    "    # grab the dates\n",
    "    d1 = row.ds1; d2 = row.ds2\n",
    "    \n",
    "    # grab the part of the df matching those dates\n",
    "    date_df = sorted_df[sorted_df.ds1 == d1]\n",
    "    date_df = date_df[date_df.ds2 == d2]\n",
    "    date_df.reset_index(drop=True, inplace=True)\n",
    "    date_df = date_df.drop_duplicates(subset='dist_km',keep='first')\n",
    "#     print(len(date_df))\n",
    "#     if len(date_df) < 218:\n",
    "#         print(np.array(date_df.dist_km))\n",
    "\n",
    "    # append into row of speed grid\n",
    "    speed_grid[rown,:] = list(date_df.vmag) # add speed along centerline to speed_grid   \n",
    "    rown += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(date_df.dist_km)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in temporal gaps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-10-20 2016-11-13\n",
      "2016-11-13 2016-12-07\n",
      "2016-12-07 2016-12-31\n",
      "2016-12-31 2017-01-24\n",
      "2017-01-24 2017-02-17\n",
      "2017-02-17 2017-03-01\n",
      "2017-03-01 2017-03-13\n",
      "\n",
      "Gap between 2017-03-13 and 2017-05-18\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cfilespath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#             # OPTION 1: fill in gaps with Nans\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#             ds1s = np.full(np.size(dists),date1_end) \u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#             ds2s = np.full(np.size(dists),date2_start)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \n\u001b[1;32m     28\u001b[0m             \u001b[38;5;66;03m# OPTION 2: fill in gaps with optical data\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[43mcfilespath\u001b[49m): \u001b[38;5;66;03m# path to optical data\u001b[39;00m\n\u001b[1;32m     30\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprofile\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[1;32m     31\u001b[0m                     dstr1 \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m8\u001b[39m:\u001b[38;5;241m16\u001b[39m]; dstr2 \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m17\u001b[39m:\u001b[38;5;241m25\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cfilespath' is not defined"
     ]
    }
   ],
   "source": [
    "# intersperse ds1 and ds2\n",
    "date_ends = np.array(list((zip(df2.ds1, df2.ds2)))).flatten()\n",
    "\n",
    "fill_dfs = []\n",
    "counter = 0\n",
    "# identify number of gaps in to fill in data\n",
    "for i in np.arange(0, len(date_ends),2):\n",
    "    if i+2 < len(date_ends):\n",
    "        # grab the two dates for that speed_grid()\n",
    "        date1_start = date_ends[i]\n",
    "        date1_end = date_ends[i+1]\n",
    "        date2_start = date_ends[i+2]\n",
    "        print(str(date1_start)[0:10],str(date1_end)[0:10]) # print the date start and end\n",
    "        \n",
    "        if not date1_end == date2_start: # if the end date and the next start date don't match\n",
    "            print()\n",
    "            print('Gap between', str(date1_end)[0:10], 'and', str(date2_start)[0:10])\n",
    "            print()\n",
    "            \n",
    "#             # OPTION 1: fill in gaps with Nans\n",
    "#             ds1s = np.full(np.size(dists),date1_end) \n",
    "#             ds2s = np.full(np.size(dists),date2_start)\n",
    "#             nans = np.empty(len(dists)); nans[:] = np.nan # create list of nans to fill\n",
    "#             fill_df = pd.DataFrame(list(zip(nans, nans, nans, nans, nans, nans, dists, nans, ds1s, ds2s)),\n",
    "#                                    columns=sorted_df.columns)\n",
    "#             fill_dfs.append(fill_df)\n",
    "            \n",
    "            # OPTION 2: fill in gaps with optical data\n",
    "            for file in os.listdir(cfilespath): # path to optical data\n",
    "                if file.startswith('profile') and file.endswith('.csv') and 'S' not in file:\n",
    "                    dstr1 = file[8:16]; dstr2 = file[17:25]\n",
    "                    ds1 = pd.to_datetime(dstr1,format='%Y%m%d')\n",
    "                    ds2 = pd.to_datetime(dstr2,format='%Y%m%d')\n",
    "                    \n",
    "                    # find optical data between the gap dates\n",
    "                    if (ds1 >= date1_end) and (ds2 <= date2_start): \n",
    "                        print('Fill with optical data :',dstr1, dstr2)\n",
    "                        profile_df = pd.read_csv(cfilespath+file, names=['dist_km','vmag_md'])\n",
    "                        \n",
    "                        # down-sample the optical data\n",
    "                        maxdist = np.nanmax(profile_df.dist_km)\n",
    "                        if  maxdist < 29.54666: # distance should go out to 29.54 km\n",
    "                            newdistkm = np.flip(np.array(profile_df.dist_km * 29.54666/maxdist)) # rescale to 29.54 km\n",
    "                            profile_df.dist_km = newdistkm # replace in dataframe\n",
    "                        sample_indices = np.round(np.linspace(0,len(profile_df)-1,40))\n",
    "                        v_interp = np.flip(profile_df.vmag_md[sample_indices])\n",
    "\n",
    "                        # fill in a ds1 and ds2 columns\n",
    "                        ds1s = np.full(np.size(v_interp),ds1) \n",
    "                        ds2s = np.full(np.size(v_interp),ds2)\n",
    "                        nans = np.empty(np.size(v_interp)); nans[:] = np.nan # create list of nans to fill\n",
    "\n",
    "                        # enter into dataframe\n",
    "                        fill_df = pd.DataFrame(list(zip(nans, nans, nans, nans, v_interp, nans, dists, nans, ds1s, ds2s)),\n",
    "                                           columns=sorted_df.columns)\n",
    "                        fill_dfs.append(fill_df)\n",
    "            counter+=1\n",
    "print(counter, 'gaps')\n",
    "\n",
    "# return the filled df\n",
    "interp_total_filled = pd.concat([sorted_df,pd.concat(fill_dfs)]).sort_values(by=['ds1','ds2','dist_km'])\n",
    "interp_total_filled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL ALL REMAINING GAPS IF OPTICAL DATA ARE INPUT\n",
    "df2 = pd.DataFrame(list(OrderedSet(zip(interp_total_filled.ds1, interp_total_filled.ds2))),\n",
    "                   columns=['ds1','ds2'])\n",
    "date_ends = np.array(list((zip(df2.ds1, df2.ds2)))).flatten()\n",
    "\n",
    "fill_dfs = []\n",
    "counter = 0\n",
    "# identify number of gaps in to fill in data\n",
    "for i in np.arange(0, len(date_ends),2):\n",
    "    if i+2 < len(date_ends):\n",
    "        # grab the two dates for that speed_grid()\n",
    "        date1_start = date_ends[i]\n",
    "        date1_end = date_ends[i+1]\n",
    "        date2_start = date_ends[i+2]\n",
    "        print(date1_start, date1_end)\n",
    "        \n",
    "        if not date1_end == date2_start: # if the end date and the next start date don't match\n",
    "            print()\n",
    "            print('Gap between', date1_end, 'and', date2_start)\n",
    "            print()\n",
    "            \n",
    "            # fill in a the gaps with nans\n",
    "            ds1s = np.full(np.size(dists),date1_end) \n",
    "            ds2s = np.full(np.size(dists),date2_start)\n",
    "            nans = np.empty(len(dists)); nans[:] = np.nan # create list of nans to fill\n",
    "            fill_df = pd.DataFrame(list(zip(nans, nans, nans, nans, nans, nans, dists, nans, ds1s, ds2s)),\n",
    "                                   columns=sorted_df.columns)\n",
    "            fill_dfs.append(fill_df)\n",
    "            counter+=1  \n",
    "print(counter, 'gaps')\n",
    "\n",
    "interp_total_filled = pd.concat([interp_total_filled,pd.concat(fill_dfs)]).sort_values(by=['ds1','ds2','dist_km'])\n",
    "interp_total_filled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Plot speed evolution time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds1</th>\n",
       "      <th>ds2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>2013-06-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-06-10</td>\n",
       "      <td>2013-06-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-06-17</td>\n",
       "      <td>2013-07-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-07-12</td>\n",
       "      <td>2013-07-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-07-28</td>\n",
       "      <td>2013-08-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>2021-08-15</td>\n",
       "      <td>2021-08-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>2021-08-30</td>\n",
       "      <td>2021-09-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>2021-09-07</td>\n",
       "      <td>2021-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>2021-09-19</td>\n",
       "      <td>2021-09-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>2021-09-22</td>\n",
       "      <td>2021-10-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>277 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ds1        ds2\n",
       "0   2013-04-14 2013-06-10\n",
       "1   2013-06-10 2013-06-17\n",
       "2   2013-06-17 2013-07-12\n",
       "3   2013-07-12 2013-07-28\n",
       "4   2013-07-28 2013-08-13\n",
       "..         ...        ...\n",
       "272 2021-08-15 2021-08-30\n",
       "273 2021-08-30 2021-09-07\n",
       "274 2021-09-07 2021-09-19\n",
       "275 2021-09-19 2021-09-22\n",
       "276 2021-09-22 2021-10-04\n",
       "\n",
       "[277 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grab ordered dates as a dataeframe\n",
    "df2_filled = unique_date_df(interp_total_filled,'ds1','ds2')\n",
    "df2_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(277, 121)\n"
     ]
    }
   ],
   "source": [
    "# Re-create speed grid with gap-filled data\n",
    "speed_grid_filled = np.zeros((len(df2_filled), len(distances)))\n",
    "\n",
    "# speed grid with gap-filled data\n",
    "rown = 0\n",
    "for idx, row in df2_filled.iterrows():\n",
    "    d1 = row.ds1; d2 = row.ds2 # grab the dates\n",
    "    \n",
    "    # grab the part of the df matching those dates\n",
    "    date_df = interp_total_filled[interp_total_filled.ds1 == d1]\n",
    "    date_df = date_df[date_df.ds2 == d2]\n",
    "    date_df.reset_index(drop=True, inplace=True)\n",
    "    date_df = date_df.drop_duplicates(subset=['dist_km'],keep='last') # drop duplications\n",
    "    \n",
    "    # append into row of speed grid\n",
    "    speed_grid_filled[rown,:] = list(date_df.vmag) # add speed along centerline to speed_grid\n",
    "    rown += 1\n",
    "print(speed_grid_filled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mr/7f69f7y50zj85n420pyvh8cr0000gq/T/ipykernel_43039/2696162903.py:2: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  spacing = np.round(np.diff(df2_filled.ds1)/np.min(np.diff(df2_filled.ds1)))\n",
      "/var/folders/mr/7f69f7y50zj85n420pyvh8cr0000gq/T/ipykernel_43039/2696162903.py:2: RuntimeWarning: invalid value encountered in true_divide\n",
      "  spacing = np.round(np.diff(df2_filled.ds1)/np.min(np.diff(df2_filled.ds1)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
       "       inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
       "       inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
       "       inf, inf, nan, inf, nan, inf, inf, nan, inf, inf, inf, nan, inf,\n",
       "       nan, inf, nan, inf, nan, inf, inf, inf, nan, inf, inf, inf, inf,\n",
       "       inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
       "       inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, nan, inf,\n",
       "       nan, inf, nan, inf, nan, inf, inf, inf, inf, inf, inf, inf, inf,\n",
       "       inf, inf, nan, inf, nan, inf, nan, inf, nan, inf, inf, nan, inf,\n",
       "       inf, nan, nan, nan, inf, nan, inf, nan, inf, nan, inf, inf, nan,\n",
       "       inf, inf, nan, inf, nan, nan, inf, nan, inf, inf, inf, inf, inf,\n",
       "       nan, inf, nan, inf, nan, inf, nan, inf, inf, inf, inf, inf, inf,\n",
       "       inf, nan, inf, nan, inf, nan, inf, nan, inf, nan, inf, nan, inf,\n",
       "       nan, nan, inf, nan, inf, nan, nan, inf, nan, inf, inf, inf, nan,\n",
       "       inf, inf, inf, inf, nan, inf, inf, inf, inf, inf, nan, nan, inf,\n",
       "       nan, inf, inf, nan, inf, nan, nan, inf, nan, inf, inf, inf, nan,\n",
       "       inf, nan, inf, nan, nan, inf, nan, inf, nan, nan, inf, inf, nan,\n",
       "       inf, nan, nan, nan, inf, nan, inf, inf, inf, inf, inf, inf, inf,\n",
       "       inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, nan, inf, nan,\n",
       "       inf, nan, inf, inf, inf, nan, inf, inf, inf, inf, inf, inf, inf,\n",
       "       inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
       "       inf, inf, inf, inf])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine the grid spacing based on the time differences\n",
    "spacing = np.round(np.diff(df2_filled.ds1)/np.min(np.diff(df2_filled.ds1)))\n",
    "spacing = np.append(spacing, spacing[-1]) # add last spacing\n",
    "spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "repeats may not contain negative values.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# create spaced grid\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m speed_spaced \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspeed_grid_filled\u001b[49m\u001b[43m,\u001b[49m\u001b[43mspacing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# along rows\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# speed_spaced[speed_spaced == 0] = np.NaN\u001b[39;00m\n\u001b[1;32m      4\u001b[0m speed_spaced\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mrepeat\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/newautoriftenv/lib/python3.9/site-packages/numpy/core/fromnumeric.py:479\u001b[0m, in \u001b[0;36mrepeat\u001b[0;34m(a, repeats, axis)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_repeat_dispatcher)\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrepeat\u001b[39m(a, repeats, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;124;03m    Repeat elements of an array.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    477\u001b[0m \n\u001b[1;32m    478\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrepeat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/newautoriftenv/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mValueError\u001b[0m: repeats may not contain negative values."
     ]
    }
   ],
   "source": [
    "# create spaced grid\n",
    "speed_spaced = np.repeat(speed_grid_filled,spacing.astype(int),0) # along rows\n",
    "# speed_spaced[speed_spaced == 0] = np.NaN\n",
    "speed_spaced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User input to properly generate axis labels for the plot:\n",
    "# if grid tick spacing is unknown, run the cell below with the tick labels turned off to see tick spacing\n",
    "\n",
    "# create centerline (x-axis) labels \n",
    "x = np.linspace(dists[0], dists[-1], speed_grid.shape[1]+1)\n",
    "x_labels = np.insert(x[::10],0,0).astype(int) # INPUT GRID TICK SPACING FOR X (e.g. 10 or 5)\n",
    "print(x_labels)\n",
    "\n",
    "# create y-axis labels\n",
    "t = pd.date_range(start='2013-06-10',end='2021-09-22',periods=len(speed_spaced)) # INPUT START AND END DATE\n",
    "y = np.insert(t[::500],0,t[0]) # INPUT GRID TICK SPACING (e.g. 250, 200, 500, or 20)\n",
    "y_labels = [ystr[:7] for ystr in y.astype(str)] # grab the first 7 digits YYYY-MM for each timestamp in y\n",
    "print(y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the properly-spaced speed evolution map with imshow\n",
    "fig,ax = plt.subplots(1,1,figsize=(6,6))\n",
    "ax.set_facecolor('black')\n",
    "grid = plt.imshow(speed_spaced,aspect=0.03, # 0.03 for full 2013-2022\n",
    "                  cmap=cmocean.cm.thermal,\n",
    "#                  vmin=0, vmax=7) # linear coloring\n",
    "                  norm=LogNorm(vmin=1, vmax=25)) # log norm coloring\n",
    "fig.colorbar(grid, orientation=\"vertical\",label=\"Surface speed (m/day)\")\n",
    "\n",
    "# set tick labels\n",
    "ax.set_xticklabels(x_labels); plt.xlabel('Distance from terminus (km)')\n",
    "ax.set_yticklabels(y_labels)\n",
    "\n",
    "# plt.savefig(basepath+\"figures/Optical_filled_with_SAR.png\",dpi=300) # save figure\n",
    "# plt.title('SAR and Optical') # add title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds1</th>\n",
       "      <th>ds2</th>\n",
       "      <th>dist_km</th>\n",
       "      <th>vmag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-04-27</td>\n",
       "      <td>2020-05-10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-04-27</td>\n",
       "      <td>2020-05-10</td>\n",
       "      <td>0.35</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-04-27</td>\n",
       "      <td>2020-05-10</td>\n",
       "      <td>0.60</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-04-27</td>\n",
       "      <td>2020-05-10</td>\n",
       "      <td>0.85</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-04-27</td>\n",
       "      <td>2020-05-10</td>\n",
       "      <td>1.10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>2020-04-27</td>\n",
       "      <td>2020-05-10</td>\n",
       "      <td>29.10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>2020-04-27</td>\n",
       "      <td>2020-05-10</td>\n",
       "      <td>29.35</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>2020-04-27</td>\n",
       "      <td>2020-05-10</td>\n",
       "      <td>29.60</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>2020-04-27</td>\n",
       "      <td>2020-05-10</td>\n",
       "      <td>29.85</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>2020-04-27</td>\n",
       "      <td>2020-05-10</td>\n",
       "      <td>30.10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ds1        ds2  dist_km  vmag\n",
       "0   2020-04-27 2020-05-10     0.10   NaN\n",
       "1   2020-04-27 2020-05-10     0.35   NaN\n",
       "2   2020-04-27 2020-05-10     0.60   NaN\n",
       "3   2020-04-27 2020-05-10     0.85   NaN\n",
       "4   2020-04-27 2020-05-10     1.10   NaN\n",
       "..         ...        ...      ...   ...\n",
       "116 2020-04-27 2020-05-10    29.10   NaN\n",
       "117 2020-04-27 2020-05-10    29.35   NaN\n",
       "118 2020-04-27 2020-05-10    29.60   NaN\n",
       "119 2020-04-27 2020-05-10    29.85   NaN\n",
       "120 2020-04-27 2020-05-10    30.10   NaN\n",
       "\n",
       "[121 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newautoriftenv",
   "language": "python",
   "name": "newautoriftenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
